---
title: "I'm scared of AGI"
date: 2024-02-26T10:45:00+01:00
draft: true
---

What convinced me that I should worry about the progress we're making on LLMs, and AI in general, was the now infamous episode "We're All Gonna Die with Eliezer Yudkowsky" on the Bankless podcast. I know this is very AI-doomer. And I wish that I wasn't convinced, but here we are.

First of all, I agree with Sam Altman that the potential upsides of building a good AGI are so bonkers that you'll sound like a madman if you spoke about it honestly. In other words: we won't be able to distinguish the capabilities of a true AGI from magic. This point is important because the AGIs we build will likely have the same capabilities whether it's helpful or antagonistic.

The argument that convinced me is the following:

1) We develop AIs today by optimizing the weights of a neural network with respect to a reward function.
2) We have seen exactly _one_ general intelligence emerge from an optimization process with respect to a reward function. Humans emerged from biological evolution. And we are a general intelligence and evolution can be modelled as a process that optimizes biological creatures with respect to mutually inclusive genetic fitness.
3) A priori, it is *highly* unintuitive that our motivations are so varied given the direct goal of optimizing mutually inclusive genetic fitness. In some cases, our motivations even seem directly counter to that goal, such as when we use contraception or when all men don't volunteer at the sperm bank.

What these facts tell me, is that we're not guaranteed that the AIs we're building will want anything intuitively close to what we're optimizing them for. But also, extrapolating from the single data point we do have, we should actually probably expect some of the motivations to be directly counter to what we're optimizing them for. And it's this last point here that scares me. We can optimize them for their want and ability to help out humanity. But we will probably end up with an entity that has wants in directly the opposite direction.